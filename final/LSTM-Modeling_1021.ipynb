{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import torch\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import scipy as sp\n",
    "import scipy.stats as sps\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Prepare Data\n",
    "def make_batch(data, batch_size, window_size, shuffle=True):\n",
    "    window_list = []\n",
    "    for i in range(len(data) - window_size - 1):\n",
    "        window = data[i: i + window_size]\n",
    "        window_list.append(window)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(window_list)\n",
    "\n",
    "    n_batch = math.ceil(len(window_list) / batch_size)\n",
    "    batch_list = []\n",
    "    for i in range(n_batch):\n",
    "        batch = window_list[i*batch_size: (i+1)*batch_size]\n",
    "        batch_list.append(batch)\n",
    "    batch_list = np.array(batch_list)\n",
    "\n",
    "    return batch_list\n",
    "#data = pd.read_csv(\"./all_metric_sort_err.csv\", sep=',', index_col=False)\n",
    "data = pd.read_csv(\"./all_metric_sort_err.csv\", sep=',', index_col=False)\n",
    "data = data.dropna(axis=0)\n",
    "p = data.isnull().values.any()\n",
    "\n",
    "headers = [h for h in data.keys() if not h.startswith('container_') and not h.startswith(\"go_\") and not h.startswith(\"node_filesystem_\") and h in forhead.keys()]\n",
    "data = data[headers]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_boot_time_seconds</th>\n",
       "      <th>node_boot_time_seconds.1</th>\n",
       "      <th>node_context_switches_total</th>\n",
       "      <th>node_context_switches_total.1</th>\n",
       "      <th>node_cpu_frequency_max_hertz</th>\n",
       "      <th>node_cpu_frequency_max_hertz.1</th>\n",
       "      <th>node_cpu_frequency_max_hertz.2</th>\n",
       "      <th>node_cpu_frequency_max_hertz.3</th>\n",
       "      <th>node_cpu_frequency_max_hertz.4</th>\n",
       "      <th>node_cpu_frequency_max_hertz.5</th>\n",
       "      <th>...</th>\n",
       "      <th>node_softnet_processed_total.16</th>\n",
       "      <th>node_softnet_processed_total.17</th>\n",
       "      <th>node_softnet_processed_total.18</th>\n",
       "      <th>node_softnet_processed_total.19</th>\n",
       "      <th>node_softnet_processed_total.20</th>\n",
       "      <th>node_softnet_processed_total.21</th>\n",
       "      <th>node_softnet_processed_total.22</th>\n",
       "      <th>node_softnet_processed_total.23</th>\n",
       "      <th>node_softnet_processed_total.24</th>\n",
       "      <th>node_softnet_processed_total.25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1596767999</td>\n",
       "      <td>1596781334</td>\n",
       "      <td>5758230699</td>\n",
       "      <td>37974415433</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>...</td>\n",
       "      <td>25544702</td>\n",
       "      <td>23436296</td>\n",
       "      <td>24701006</td>\n",
       "      <td>46671236</td>\n",
       "      <td>47947309</td>\n",
       "      <td>44975004</td>\n",
       "      <td>46843927</td>\n",
       "      <td>45760864</td>\n",
       "      <td>63930543</td>\n",
       "      <td>44422656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1596767999</td>\n",
       "      <td>1596781334</td>\n",
       "      <td>5758245446</td>\n",
       "      <td>37974538311</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>...</td>\n",
       "      <td>25544717</td>\n",
       "      <td>23436551</td>\n",
       "      <td>24701007</td>\n",
       "      <td>46671276</td>\n",
       "      <td>47947361</td>\n",
       "      <td>44975035</td>\n",
       "      <td>46844112</td>\n",
       "      <td>45760923</td>\n",
       "      <td>63930635</td>\n",
       "      <td>44422703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1596767999</td>\n",
       "      <td>1596781334</td>\n",
       "      <td>5758264904</td>\n",
       "      <td>37974658948</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>...</td>\n",
       "      <td>25544782</td>\n",
       "      <td>23436557</td>\n",
       "      <td>24701036</td>\n",
       "      <td>46671443</td>\n",
       "      <td>47947485</td>\n",
       "      <td>44975165</td>\n",
       "      <td>46844366</td>\n",
       "      <td>45761027</td>\n",
       "      <td>63930885</td>\n",
       "      <td>44422959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1596767999</td>\n",
       "      <td>1596781334</td>\n",
       "      <td>5758284370</td>\n",
       "      <td>37974786059</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>...</td>\n",
       "      <td>25544782</td>\n",
       "      <td>23436557</td>\n",
       "      <td>24701036</td>\n",
       "      <td>46671450</td>\n",
       "      <td>47947498</td>\n",
       "      <td>44975181</td>\n",
       "      <td>46844387</td>\n",
       "      <td>45761054</td>\n",
       "      <td>63930967</td>\n",
       "      <td>44422966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1596767999</td>\n",
       "      <td>1596781334</td>\n",
       "      <td>5758305482</td>\n",
       "      <td>37974907235</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>2100000000</td>\n",
       "      <td>...</td>\n",
       "      <td>25544858</td>\n",
       "      <td>23436564</td>\n",
       "      <td>24701043</td>\n",
       "      <td>46671924</td>\n",
       "      <td>47947955</td>\n",
       "      <td>44975292</td>\n",
       "      <td>46844543</td>\n",
       "      <td>45761183</td>\n",
       "      <td>63931182</td>\n",
       "      <td>44423190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1578 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   node_boot_time_seconds  node_boot_time_seconds.1  \\\n",
       "0              1596767999                1596781334   \n",
       "1              1596767999                1596781334   \n",
       "2              1596767999                1596781334   \n",
       "3              1596767999                1596781334   \n",
       "4              1596767999                1596781334   \n",
       "\n",
       "   node_context_switches_total  node_context_switches_total.1  \\\n",
       "0                   5758230699                    37974415433   \n",
       "1                   5758245446                    37974538311   \n",
       "2                   5758264904                    37974658948   \n",
       "3                   5758284370                    37974786059   \n",
       "4                   5758305482                    37974907235   \n",
       "\n",
       "   node_cpu_frequency_max_hertz  node_cpu_frequency_max_hertz.1  \\\n",
       "0                    2100000000                      2100000000   \n",
       "1                    2100000000                      2100000000   \n",
       "2                    2100000000                      2100000000   \n",
       "3                    2100000000                      2100000000   \n",
       "4                    2100000000                      2100000000   \n",
       "\n",
       "   node_cpu_frequency_max_hertz.2  node_cpu_frequency_max_hertz.3  \\\n",
       "0                      2100000000                      2100000000   \n",
       "1                      2100000000                      2100000000   \n",
       "2                      2100000000                      2100000000   \n",
       "3                      2100000000                      2100000000   \n",
       "4                      2100000000                      2100000000   \n",
       "\n",
       "   node_cpu_frequency_max_hertz.4  node_cpu_frequency_max_hertz.5  ...  \\\n",
       "0                      2100000000                      2100000000  ...   \n",
       "1                      2100000000                      2100000000  ...   \n",
       "2                      2100000000                      2100000000  ...   \n",
       "3                      2100000000                      2100000000  ...   \n",
       "4                      2100000000                      2100000000  ...   \n",
       "\n",
       "   node_softnet_processed_total.16  node_softnet_processed_total.17  \\\n",
       "0                         25544702                         23436296   \n",
       "1                         25544717                         23436551   \n",
       "2                         25544782                         23436557   \n",
       "3                         25544782                         23436557   \n",
       "4                         25544858                         23436564   \n",
       "\n",
       "   node_softnet_processed_total.18  node_softnet_processed_total.19  \\\n",
       "0                         24701006                         46671236   \n",
       "1                         24701007                         46671276   \n",
       "2                         24701036                         46671443   \n",
       "3                         24701036                         46671450   \n",
       "4                         24701043                         46671924   \n",
       "\n",
       "   node_softnet_processed_total.20  node_softnet_processed_total.21  \\\n",
       "0                         47947309                         44975004   \n",
       "1                         47947361                         44975035   \n",
       "2                         47947485                         44975165   \n",
       "3                         47947498                         44975181   \n",
       "4                         47947955                         44975292   \n",
       "\n",
       "   node_softnet_processed_total.22  node_softnet_processed_total.23  \\\n",
       "0                         46843927                         45760864   \n",
       "1                         46844112                         45760923   \n",
       "2                         46844366                         45761027   \n",
       "3                         46844387                         45761054   \n",
       "4                         46844543                         45761183   \n",
       "\n",
       "   node_softnet_processed_total.24  node_softnet_processed_total.25  \n",
       "0                         63930543                         44422656  \n",
       "1                         63930635                         44422703  \n",
       "2                         63930885                         44422959  \n",
       "3                         63930967                         44422966  \n",
       "4                         63931182                         44423190  \n",
       "\n",
       "[5 rows x 1578 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to_numpy()\n",
    "#scaler = MinMaxScaler(feature_range=(0,10))\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Modeling\n",
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self, input_size=1796, output_dim=1796, hidden_size=256, num_layers=1):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.scaler_bias = nn.Parameter(torch.ones(input_size, requires_grad=True))\n",
    "        self.scaler = nn.Parameter(torch.ones(input_size, requires_grad=True))\n",
    "        self.linear = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x + self.scaler_bias) * self.scaler\n",
    "        zs, hidden = self.lstm(x)\n",
    "        z = zs[:, -1]\n",
    "        v = self.linear(zs)\n",
    "        return v, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Training\n",
    "window_size = 10\n",
    "batch_size = 192\n",
    "hidden_size = 128\n",
    "use_cuda = True\n",
    "n_feature = data.shape[1]\n",
    "\n",
    "model = SequenceModel(input_size=n_feature,\n",
    "                      output_dim=n_feature,\n",
    "                      hidden_size=hidden_size,\n",
    "                      num_layers=1)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "n_epoch = 30000\n",
    "ema_loss = None\n",
    "alpha = 0.1\n",
    "verbose_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch, loss: 0.3837843775749206\n",
      "50th epoch, loss: 0.08462249602843495\n",
      "100th epoch, loss: 0.06674098407573809\n",
      "150th epoch, loss: 0.059712057492054646\n",
      "200th epoch, loss: 0.055790387245457415\n",
      "250th epoch, loss: 0.05216448614928952\n",
      "300th epoch, loss: 0.04888100172160663\n",
      "350th epoch, loss: 0.04598041778995166\n",
      "400th epoch, loss: 0.04342616852078391\n",
      "450th epoch, loss: 0.04157690207308597\n",
      "500th epoch, loss: 0.03927909474817022\n",
      "550th epoch, loss: 0.037651970576966884\n",
      "600th epoch, loss: 0.03547101883491353\n",
      "650th epoch, loss: 0.03387561708515976\n",
      "700th epoch, loss: 0.032414921212774624\n",
      "750th epoch, loss: 0.030822242195357365\n",
      "800th epoch, loss: 0.029610076468130943\n",
      "850th epoch, loss: 0.028681178376936647\n",
      "900th epoch, loss: 0.027962448740616055\n",
      "950th epoch, loss: 0.02713092194448657\n",
      "1000th epoch, loss: 0.02629550804656317\n",
      "1050th epoch, loss: 0.025684502575872594\n",
      "1100th epoch, loss: 0.025037591485649345\n",
      "1150th epoch, loss: 0.0245267129750576\n",
      "1200th epoch, loss: 0.023828865255412134\n",
      "1250th epoch, loss: 0.02326968545115849\n",
      "1300th epoch, loss: 0.023050696090383464\n",
      "1350th epoch, loss: 0.02226020301946931\n",
      "1400th epoch, loss: 0.021616423436421655\n",
      "1450th epoch, loss: 0.021032385358123375\n",
      "1500th epoch, loss: 0.020832848809315952\n",
      "1550th epoch, loss: 0.020480611175152343\n",
      "1600th epoch, loss: 0.02018200824017497\n",
      "1650th epoch, loss: 0.019846775459048754\n",
      "1700th epoch, loss: 0.01972042812848702\n",
      "1750th epoch, loss: 0.019138357719560066\n",
      "1800th epoch, loss: 0.019215674654765803\n",
      "1850th epoch, loss: 0.018598391979053806\n",
      "1900th epoch, loss: 0.018636018524292877\n",
      "1950th epoch, loss: 0.0181932901344704\n",
      "2000th epoch, loss: 0.01820529968697838\n",
      "2050th epoch, loss: 0.018231669987052397\n",
      "2100th epoch, loss: 0.017610642441146407\n",
      "2150th epoch, loss: 0.017556325075388676\n",
      "2200th epoch, loss: 0.017371117602849102\n",
      "2250th epoch, loss: 0.017420656635078836\n",
      "2300th epoch, loss: 0.01717283134244038\n",
      "2350th epoch, loss: 0.016771062425508097\n",
      "2400th epoch, loss: 0.016719647964007454\n",
      "2450th epoch, loss: 0.01629583818389584\n",
      "2500th epoch, loss: 0.015986706488935565\n",
      "2550th epoch, loss: 0.01576285773341593\n",
      "2600th epoch, loss: 0.015850398412618885\n",
      "2650th epoch, loss: 0.015577001843093002\n",
      "2700th epoch, loss: 0.015506050587902803\n",
      "2750th epoch, loss: 0.015450694807425773\n",
      "2800th epoch, loss: 0.01513646830479245\n",
      "2850th epoch, loss: 0.015655957150857307\n",
      "2900th epoch, loss: 0.014847525112509821\n",
      "2950th epoch, loss: 0.014794361763569923\n",
      "3000th epoch, loss: 0.014720196632341014\n",
      "3050th epoch, loss: 0.014762594664004735\n",
      "3100th epoch, loss: 0.014672830729057944\n",
      "3150th epoch, loss: 0.01439420621171409\n",
      "3200th epoch, loss: 0.014498730197848784\n",
      "3250th epoch, loss: 0.014139782071627433\n",
      "3300th epoch, loss: 0.014063299070273956\n",
      "3350th epoch, loss: 0.014054035736396787\n",
      "3400th epoch, loss: 0.013907835512238266\n",
      "3450th epoch, loss: 0.01377139566857452\n",
      "3500th epoch, loss: 0.013662393974369108\n",
      "3550th epoch, loss: 0.013391998009941617\n",
      "3600th epoch, loss: 0.01344440762428363\n",
      "3650th epoch, loss: 0.013226094442125502\n",
      "3700th epoch, loss: 0.01288455984595201\n",
      "3750th epoch, loss: 0.01272521526486252\n",
      "3800th epoch, loss: 0.012639983692968017\n",
      "3850th epoch, loss: 0.012626529876801004\n",
      "3900th epoch, loss: 0.01250909828844961\n",
      "3950th epoch, loss: 0.012563812582605367\n",
      "4000th epoch, loss: 0.012406408035038159\n",
      "4050th epoch, loss: 0.011897749634192389\n",
      "4100th epoch, loss: 0.012031435743123027\n",
      "4150th epoch, loss: 0.012077214757831878\n",
      "4200th epoch, loss: 0.011728775903731092\n",
      "4250th epoch, loss: 0.01167717230685857\n",
      "4300th epoch, loss: 0.01186336465741351\n",
      "4350th epoch, loss: 0.011608087298382235\n",
      "4400th epoch, loss: 0.011555329305037122\n",
      "4450th epoch, loss: 0.011396092641557703\n",
      "4500th epoch, loss: 0.011626800986274773\n",
      "4550th epoch, loss: 0.011524581709432666\n",
      "4600th epoch, loss: 0.011528163941013195\n",
      "4650th epoch, loss: 0.011338607827677871\n",
      "4700th epoch, loss: 0.01138894872527178\n",
      "4750th epoch, loss: 0.011204009583883437\n",
      "4800th epoch, loss: 0.011023336015843713\n",
      "4850th epoch, loss: 0.011221476510919636\n",
      "4900th epoch, loss: 0.011202232097804906\n",
      "4950th epoch, loss: 0.010919584065860029\n",
      "5000th epoch, loss: 0.011070523996808778\n",
      "5050th epoch, loss: 0.011225899187799055\n",
      "5100th epoch, loss: 0.01086741400081656\n",
      "5150th epoch, loss: 0.010993674104377469\n",
      "5200th epoch, loss: 0.010798078064034385\n",
      "5250th epoch, loss: 0.010847493173724264\n",
      "5300th epoch, loss: 0.010549618113219085\n",
      "5350th epoch, loss: 0.010613270364973918\n",
      "5400th epoch, loss: 0.0107839183013339\n",
      "5450th epoch, loss: 0.010581295469907694\n",
      "5500th epoch, loss: 0.01065838490843261\n",
      "5550th epoch, loss: 0.010489401378807219\n",
      "5600th epoch, loss: 0.010637994502805987\n",
      "5650th epoch, loss: 0.010496442659106939\n",
      "5700th epoch, loss: 0.010291172888320217\n",
      "5750th epoch, loss: 0.010362467090575315\n",
      "5800th epoch, loss: 0.010613916306667295\n",
      "5850th epoch, loss: 0.01040538859446025\n",
      "5900th epoch, loss: 0.010312218823358511\n",
      "5950th epoch, loss: 0.010473460325076777\n",
      "6000th epoch, loss: 0.010296171804569898\n",
      "6050th epoch, loss: 0.010455906048408032\n",
      "6100th epoch, loss: 0.010324474760496135\n",
      "6150th epoch, loss: 0.010143801490120351\n",
      "6200th epoch, loss: 0.010467679560316714\n",
      "6250th epoch, loss: 0.01017658717812867\n",
      "6300th epoch, loss: 0.010721476810817187\n",
      "6350th epoch, loss: 0.010045936008591124\n",
      "6400th epoch, loss: 0.01002804121488973\n",
      "6450th epoch, loss: 0.010141116296811086\n",
      "6500th epoch, loss: 0.009904068831735259\n",
      "6550th epoch, loss: 0.010056394271418297\n",
      "6600th epoch, loss: 0.009816534386266248\n",
      "6650th epoch, loss: 0.010075878961570854\n",
      "6700th epoch, loss: 0.009901880618260707\n",
      "6750th epoch, loss: 0.009932118544903366\n",
      "6800th epoch, loss: 0.009886839257128075\n",
      "6850th epoch, loss: 0.010061161153046986\n",
      "6900th epoch, loss: 0.009637751398355357\n",
      "6950th epoch, loss: 0.009702608242907246\n",
      "7000th epoch, loss: 0.009637051968007179\n",
      "7050th epoch, loss: 0.009826444671366053\n",
      "7100th epoch, loss: 0.009682818653123985\n",
      "7150th epoch, loss: 0.01003341201060455\n",
      "7200th epoch, loss: 0.009423798917408452\n",
      "7250th epoch, loss: 0.009385954941206523\n",
      "7300th epoch, loss: 0.0095872825323494\n",
      "7350th epoch, loss: 0.010235537650499784\n",
      "7400th epoch, loss: 0.009212538677037939\n",
      "7450th epoch, loss: 0.009239324554284539\n",
      "7500th epoch, loss: 0.009365941683645856\n",
      "7550th epoch, loss: 0.00988638359251227\n",
      "7600th epoch, loss: 0.009219586329751306\n",
      "7650th epoch, loss: 0.009240886914372737\n",
      "7700th epoch, loss: 0.009251779565108906\n",
      "7750th epoch, loss: 0.009509714154154109\n",
      "7800th epoch, loss: 0.009282728602934187\n",
      "7850th epoch, loss: 0.008987028975307555\n",
      "7900th epoch, loss: 0.008989451202500049\n",
      "7950th epoch, loss: 0.008978254930488528\n",
      "8000th epoch, loss: 0.009454196455413665\n",
      "8050th epoch, loss: 0.009002323576950536\n",
      "8100th epoch, loss: 0.008996763202981739\n",
      "8150th epoch, loss: 0.008980399562961525\n",
      "8200th epoch, loss: 0.008876152479480547\n",
      "8250th epoch, loss: 0.009066824189080646\n",
      "8300th epoch, loss: 0.009534639016878751\n",
      "8350th epoch, loss: 0.008909307850949772\n",
      "8400th epoch, loss: 0.008806967565076195\n",
      "8450th epoch, loss: 0.008878779836196575\n",
      "8500th epoch, loss: 0.0089228666097\n",
      "8550th epoch, loss: 0.009474150038091381\n",
      "8600th epoch, loss: 0.00919271267172529\n",
      "8650th epoch, loss: 0.008762688057598054\n",
      "8700th epoch, loss: 0.00885269112991831\n",
      "8750th epoch, loss: 0.008845721615666692\n",
      "8800th epoch, loss: 0.008988171125199857\n",
      "8850th epoch, loss: 0.008754548560106306\n",
      "8900th epoch, loss: 0.008685510949873725\n",
      "8950th epoch, loss: 0.008836793067086582\n",
      "9000th epoch, loss: 0.008922288371128195\n",
      "9050th epoch, loss: 0.008904536076653572\n",
      "9100th epoch, loss: 0.008563938578230708\n",
      "9150th epoch, loss: 0.008730592188439985\n",
      "9200th epoch, loss: 0.008625378392162819\n",
      "9250th epoch, loss: 0.008986674651092326\n",
      "9300th epoch, loss: 0.008333734379809095\n",
      "9350th epoch, loss: 0.008397591362194924\n",
      "9400th epoch, loss: 0.008691503267004888\n",
      "9450th epoch, loss: 0.008785524341595108\n",
      "9500th epoch, loss: 0.00843483462065221\n",
      "9550th epoch, loss: 0.008370274113895439\n",
      "9600th epoch, loss: 0.00856676559423403\n",
      "9650th epoch, loss: 0.008315315365731124\n",
      "9700th epoch, loss: 0.008342230557892601\n",
      "9750th epoch, loss: 0.008317593147464802\n",
      "9800th epoch, loss: 0.008492699086385457\n",
      "9850th epoch, loss: 0.008355965615639475\n",
      "9900th epoch, loss: 0.008173155393966396\n",
      "9950th epoch, loss: 0.008290239112472682\n",
      "10000th epoch, loss: 0.008373597991511545\n",
      "10050th epoch, loss: 0.008150748297994892\n",
      "10100th epoch, loss: 0.008256786283222956\n",
      "10150th epoch, loss: 0.008362225932054877\n",
      "10200th epoch, loss: 0.00813595519910313\n",
      "10250th epoch, loss: 0.008255813803663543\n",
      "10300th epoch, loss: 0.008405949363940679\n",
      "10350th epoch, loss: 0.008486250751802496\n",
      "10400th epoch, loss: 0.008075851039563355\n",
      "10450th epoch, loss: 0.008045045693349271\n",
      "10500th epoch, loss: 0.008043815037943866\n",
      "10550th epoch, loss: 0.008273564949172843\n",
      "10600th epoch, loss: 0.008076897968045843\n",
      "10650th epoch, loss: 0.008174774986722448\n",
      "10700th epoch, loss: 0.008156501905708128\n",
      "10750th epoch, loss: 0.008137803487148884\n",
      "10800th epoch, loss: 0.008004306599914912\n",
      "10850th epoch, loss: 0.00806723243011355\n",
      "10900th epoch, loss: 0.008068141298816004\n",
      "10950th epoch, loss: 0.008103683003327405\n",
      "11000th epoch, loss: 0.008155600494018325\n",
      "11050th epoch, loss: 0.00798445217927308\n",
      "11100th epoch, loss: 0.008268021503206\n",
      "11150th epoch, loss: 0.008102001134287606\n",
      "11200th epoch, loss: 0.007997291649713297\n",
      "11250th epoch, loss: 0.007903642543252083\n",
      "11300th epoch, loss: 0.00798966637933877\n",
      "11350th epoch, loss: 0.00806852866206377\n",
      "11400th epoch, loss: 0.007865209582493732\n",
      "11450th epoch, loss: 0.008001134114069367\n",
      "11500th epoch, loss: 0.007911157862950971\n",
      "11550th epoch, loss: 0.008171792339363595\n",
      "11600th epoch, loss: 0.00780522723344993\n",
      "11650th epoch, loss: 0.00798115424898938\n",
      "11700th epoch, loss: 0.007842536567832908\n",
      "11750th epoch, loss: 0.007895112773940201\n",
      "11800th epoch, loss: 0.007727648369798095\n",
      "11850th epoch, loss: 0.008051153708606608\n",
      "11900th epoch, loss: 0.007736317524693898\n",
      "11950th epoch, loss: 0.007810721970544527\n",
      "12000th epoch, loss: 0.007975376252976205\n",
      "12050th epoch, loss: 0.007875789733956985\n",
      "12100th epoch, loss: 0.007658601052566316\n",
      "12150th epoch, loss: 0.007664635054645218\n",
      "12200th epoch, loss: 0.00772490927287549\n",
      "12250th epoch, loss: 0.007713304628203968\n",
      "12300th epoch, loss: 0.007811985833749485\n",
      "12350th epoch, loss: 0.007958844717291547\n",
      "12400th epoch, loss: 0.007795576077822457\n",
      "12450th epoch, loss: 0.007822915456802734\n",
      "12500th epoch, loss: 0.007799318840409342\n",
      "12550th epoch, loss: 0.007731422770136419\n",
      "12600th epoch, loss: 0.007704498886499135\n",
      "12650th epoch, loss: 0.007636069186568258\n",
      "12700th epoch, loss: 0.007974197244568004\n",
      "12750th epoch, loss: 0.007676648214404282\n",
      "12800th epoch, loss: 0.008082678490631321\n",
      "12850th epoch, loss: 0.007883179258559282\n",
      "12900th epoch, loss: 0.007787577130321198\n",
      "12950th epoch, loss: 0.007616073511809005\n",
      "13000th epoch, loss: 0.007601971622420493\n",
      "13050th epoch, loss: 0.0075972484761677155\n",
      "13100th epoch, loss: 0.007694828278635003\n",
      "13150th epoch, loss: 0.007697370347556425\n",
      "13200th epoch, loss: 0.007507833298332771\n",
      "13250th epoch, loss: 0.007685086814105734\n",
      "13300th epoch, loss: 0.0075872988877352\n",
      "13350th epoch, loss: 0.007408059016982562\n",
      "13400th epoch, loss: 0.007716980775970048\n",
      "13450th epoch, loss: 0.0075611675168751\n",
      "13500th epoch, loss: 0.007446299409797599\n",
      "13550th epoch, loss: 0.007516116419650024\n",
      "13600th epoch, loss: 0.007477514816014989\n",
      "13650th epoch, loss: 0.007398518649310147\n",
      "13700th epoch, loss: 0.00730194495529296\n",
      "13750th epoch, loss: 0.007338272255559836\n",
      "13800th epoch, loss: 0.007244417996264939\n",
      "13850th epoch, loss: 0.007184227994683114\n",
      "13900th epoch, loss: 0.007514190010844911\n",
      "13950th epoch, loss: 0.007166084076341083\n",
      "14000th epoch, loss: 0.007236423254602472\n",
      "14050th epoch, loss: 0.0073438424273276895\n",
      "14100th epoch, loss: 0.007072889687822577\n",
      "14150th epoch, loss: 0.007195174385908166\n",
      "14200th epoch, loss: 0.007006589218753023\n",
      "14250th epoch, loss: 0.007066699276067157\n",
      "14300th epoch, loss: 0.0072942143286404935\n",
      "14350th epoch, loss: 0.007066136872573259\n",
      "14400th epoch, loss: 0.006984520590642715\n",
      "14450th epoch, loss: 0.006933936327823827\n",
      "14500th epoch, loss: 0.007011759919794489\n",
      "14550th epoch, loss: 0.007021937751558929\n",
      "14600th epoch, loss: 0.006924905082512857\n",
      "14650th epoch, loss: 0.007009479687190733\n",
      "14700th epoch, loss: 0.0070390063473647875\n",
      "14750th epoch, loss: 0.007113374995445673\n",
      "14800th epoch, loss: 0.006883690484601893\n",
      "14850th epoch, loss: 0.006924790028895509\n",
      "14900th epoch, loss: 0.007018543175841601\n",
      "14950th epoch, loss: 0.00715239731172795\n",
      "15000th epoch, loss: 0.0068024039547781535\n",
      "15050th epoch, loss: 0.007093816959338948\n",
      "15100th epoch, loss: 0.006926342387179272\n",
      "15150th epoch, loss: 0.006813019086712645\n",
      "15200th epoch, loss: 0.006911145558938547\n",
      "15250th epoch, loss: 0.006926551723697997\n",
      "15300th epoch, loss: 0.006843639839543633\n",
      "15350th epoch, loss: 0.006944650679507637\n",
      "15400th epoch, loss: 0.007051859954979292\n",
      "15450th epoch, loss: 0.006748796655714133\n",
      "15500th epoch, loss: 0.0066791084043545616\n",
      "15550th epoch, loss: 0.006708150740695154\n",
      "15600th epoch, loss: 0.006824538867902412\n",
      "15650th epoch, loss: 0.00669797456243406\n",
      "15700th epoch, loss: 0.006696371496294652\n",
      "15750th epoch, loss: 0.006705818516833621\n",
      "15800th epoch, loss: 0.006585544927245415\n",
      "15850th epoch, loss: 0.006869866472488608\n",
      "15900th epoch, loss: 0.006594570252237559\n",
      "15950th epoch, loss: 0.006726326290494952\n",
      "16000th epoch, loss: 0.007044688730200569\n",
      "16050th epoch, loss: 0.00656687111854952\n",
      "16100th epoch, loss: 0.00649967417551311\n",
      "16150th epoch, loss: 0.006511278927323353\n",
      "16200th epoch, loss: 0.006508259610724411\n",
      "16250th epoch, loss: 0.0067379922732885515\n",
      "16300th epoch, loss: 0.006504870941826115\n",
      "16350th epoch, loss: 0.00649862183867693\n",
      "16400th epoch, loss: 0.006495305656308732\n",
      "16450th epoch, loss: 0.0066068311211099845\n",
      "16500th epoch, loss: 0.0064631643158240114\n",
      "16550th epoch, loss: 0.006518951461263263\n",
      "16600th epoch, loss: 0.0064750721527868345\n",
      "16650th epoch, loss: 0.0065066461179764876\n",
      "16700th epoch, loss: 0.006563714319129088\n",
      "16750th epoch, loss: 0.006403929169200574\n",
      "16800th epoch, loss: 0.006582558251112822\n",
      "16850th epoch, loss: 0.006413183538296198\n",
      "16900th epoch, loss: 0.0063104802847766175\n",
      "16950th epoch, loss: 0.006249861918113769\n",
      "17000th epoch, loss: 0.00639852029390708\n",
      "17050th epoch, loss: 0.006252678522471622\n",
      "17100th epoch, loss: 0.006242670433361252\n",
      "17150th epoch, loss: 0.006148735078889994\n",
      "17200th epoch, loss: 0.006222589164099889\n",
      "17250th epoch, loss: 0.006964109002235306\n",
      "17300th epoch, loss: 0.006186921224985026\n",
      "17350th epoch, loss: 0.006049498455984343\n",
      "17400th epoch, loss: 0.006154449736129953\n",
      "17450th epoch, loss: 0.006193145460030761\n",
      "17500th epoch, loss: 0.006206724241616279\n",
      "17550th epoch, loss: 0.00623913617585071\n",
      "17600th epoch, loss: 0.006123603873412286\n",
      "17650th epoch, loss: 0.006001279863794637\n",
      "17700th epoch, loss: 0.0060605729312784296\n",
      "17750th epoch, loss: 0.006651943052145373\n",
      "17800th epoch, loss: 0.006080612974172261\n",
      "17850th epoch, loss: 0.006153689062599431\n",
      "17900th epoch, loss: 0.005959637412242799\n",
      "17950th epoch, loss: 0.00618180513711469\n",
      "18000th epoch, loss: 0.0060179269085854\n",
      "18050th epoch, loss: 0.006022473698594121\n",
      "18100th epoch, loss: 0.006047436337709941\n",
      "18150th epoch, loss: 0.00612943370111986\n",
      "18200th epoch, loss: 0.005968864962345465\n",
      "18250th epoch, loss: 0.0059267919867401755\n",
      "18300th epoch, loss: 0.0059381244787758735\n",
      "18350th epoch, loss: 0.005984145000538116\n",
      "18400th epoch, loss: 0.005931928938913406\n",
      "18450th epoch, loss: 0.0061618450155062005\n",
      "18500th epoch, loss: 0.005847415474373193\n",
      "18550th epoch, loss: 0.00584317524164325\n",
      "18600th epoch, loss: 0.005898997217100632\n",
      "18650th epoch, loss: 0.005838372234207604\n",
      "18700th epoch, loss: 0.005798335774313707\n",
      "18750th epoch, loss: 0.005947553409872833\n",
      "18800th epoch, loss: 0.005963554603883817\n",
      "18850th epoch, loss: 0.005711596018874441\n",
      "18900th epoch, loss: 0.005764721632449422\n",
      "18950th epoch, loss: 0.005710328062049301\n",
      "19000th epoch, loss: 0.005763594260675267\n",
      "19050th epoch, loss: 0.005902011559206038\n",
      "19100th epoch, loss: 0.00580767327976087\n",
      "19150th epoch, loss: 0.005909845682952382\n",
      "19200th epoch, loss: 0.005685204254599902\n",
      "19250th epoch, loss: 0.00579681872100828\n",
      "19300th epoch, loss: 0.006518476021254129\n",
      "19350th epoch, loss: 0.005775755283246434\n",
      "19400th epoch, loss: 0.00593832445548949\n",
      "19450th epoch, loss: 0.0056492250764450325\n",
      "19500th epoch, loss: 0.005660208549760204\n",
      "19550th epoch, loss: 0.005711629808158771\n",
      "19600th epoch, loss: 0.005651221950291506\n",
      "19650th epoch, loss: 0.0057447549947467185\n",
      "19700th epoch, loss: 0.005849781071465759\n",
      "19750th epoch, loss: 0.005675012413899825\n",
      "19800th epoch, loss: 0.005670530370130638\n",
      "19850th epoch, loss: 0.005783797871223758\n",
      "19900th epoch, loss: 0.005809649725118302\n",
      "19950th epoch, loss: 0.005882247563865019\n",
      "20000th epoch, loss: 0.00569564397556903\n",
      "20050th epoch, loss: 0.005650935257791154\n",
      "20100th epoch, loss: 0.005786725292530684\n",
      "20150th epoch, loss: 0.005717900960122573\n",
      "20200th epoch, loss: 0.00562797048736602\n",
      "20250th epoch, loss: 0.005678873094129755\n",
      "20300th epoch, loss: 0.005732203494087675\n",
      "20350th epoch, loss: 0.005799666751099319\n",
      "20400th epoch, loss: 0.005908694398513459\n",
      "20450th epoch, loss: 0.005578511280236067\n",
      "20500th epoch, loss: 0.005579188819316037\n",
      "20550th epoch, loss: 0.005571381742269475\n",
      "20600th epoch, loss: 0.00558811893408873\n",
      "20650th epoch, loss: 0.00574038358651074\n",
      "20700th epoch, loss: 0.005552722035455703\n",
      "20750th epoch, loss: 0.005573323256277702\n",
      "20800th epoch, loss: 0.005656161924185463\n",
      "20850th epoch, loss: 0.005545571699624356\n",
      "20900th epoch, loss: 0.005444518761834715\n",
      "20950th epoch, loss: 0.005454572394884303\n",
      "21000th epoch, loss: 0.0056234920252197645\n",
      "21050th epoch, loss: 0.005682008787168\n",
      "21100th epoch, loss: 0.005598972432884251\n",
      "21150th epoch, loss: 0.005471488807665555\n",
      "21200th epoch, loss: 0.005360811778336374\n",
      "21250th epoch, loss: 0.005516069275573204\n",
      "21300th epoch, loss: 0.005654453885124659\n",
      "21350th epoch, loss: 0.005392598286879483\n",
      "21400th epoch, loss: 0.005424773425461679\n",
      "21450th epoch, loss: 0.005493458530528273\n",
      "21500th epoch, loss: 0.005642281558126378\n",
      "21550th epoch, loss: 0.005555259018291459\n",
      "21600th epoch, loss: 0.005346297996082654\n",
      "21650th epoch, loss: 0.005409246227671158\n",
      "21700th epoch, loss: 0.0054163201978919065\n",
      "21750th epoch, loss: 0.005480610033037065\n",
      "21800th epoch, loss: 0.005368675821111735\n",
      "21850th epoch, loss: 0.0053876462097012575\n",
      "21900th epoch, loss: 0.005557719400948368\n",
      "21950th epoch, loss: 0.005367205799732448\n",
      "22000th epoch, loss: 0.005383278464352829\n",
      "22050th epoch, loss: 0.005504173373642831\n",
      "22100th epoch, loss: 0.005341957275050558\n",
      "22150th epoch, loss: 0.005397522477604066\n",
      "22200th epoch, loss: 0.005381130681190359\n",
      "22250th epoch, loss: 0.0054525414510956435\n",
      "22300th epoch, loss: 0.005379700314117992\n",
      "22350th epoch, loss: 0.005447581920936666\n",
      "22400th epoch, loss: 0.005326684643959288\n",
      "22450th epoch, loss: 0.005537097267762532\n",
      "22500th epoch, loss: 0.005350872769697335\n",
      "22550th epoch, loss: 0.005449417729303129\n",
      "22600th epoch, loss: 0.0053867832717251094\n",
      "22650th epoch, loss: 0.005280365216991296\n",
      "22700th epoch, loss: 0.005416238620782472\n",
      "22750th epoch, loss: 0.005233821271649295\n",
      "22800th epoch, loss: 0.00556654842206165\n",
      "22850th epoch, loss: 0.005354314255532749\n",
      "22900th epoch, loss: 0.005221921255061484\n",
      "22950th epoch, loss: 0.005238703090939143\n",
      "23000th epoch, loss: 0.005286970604900711\n",
      "23050th epoch, loss: 0.005369511739448149\n",
      "23100th epoch, loss: 0.005389894738547162\n",
      "23150th epoch, loss: 0.00528588612942274\n",
      "23200th epoch, loss: 0.005237483131024507\n",
      "23250th epoch, loss: 0.00527050452283671\n",
      "23300th epoch, loss: 0.005421965416375231\n",
      "23350th epoch, loss: 0.005192088801219907\n",
      "23400th epoch, loss: 0.005128052451376945\n",
      "23450th epoch, loss: 0.005143195911336895\n",
      "23500th epoch, loss: 0.005213605302069899\n",
      "23550th epoch, loss: 0.005145483686541761\n",
      "23600th epoch, loss: 0.005528454810979456\n",
      "23650th epoch, loss: 0.0051206753568667565\n",
      "23700th epoch, loss: 0.005098439433219312\n",
      "23750th epoch, loss: 0.005126831173248233\n",
      "23800th epoch, loss: 0.005154675235731999\n",
      "23850th epoch, loss: 0.004991285405908706\n",
      "23900th epoch, loss: 0.005085500173167797\n",
      "23950th epoch, loss: 0.005071946211458332\n",
      "24000th epoch, loss: 0.0050276535521030915\n",
      "24050th epoch, loss: 0.0050628172919683695\n",
      "24100th epoch, loss: 0.005057865813798297\n",
      "24150th epoch, loss: 0.005464392355988569\n",
      "24200th epoch, loss: 0.004937889723323076\n",
      "24250th epoch, loss: 0.005006779431735476\n",
      "24300th epoch, loss: 0.0050472637494895995\n",
      "24350th epoch, loss: 0.005093370901529346\n",
      "24400th epoch, loss: 0.004995867087951319\n",
      "24450th epoch, loss: 0.004919564681713447\n",
      "24500th epoch, loss: 0.004940881918662937\n",
      "24550th epoch, loss: 0.005063832565301327\n",
      "24600th epoch, loss: 0.005123784562165417\n",
      "24650th epoch, loss: 0.005000238272409377\n",
      "24700th epoch, loss: 0.005008046182850408\n",
      "24750th epoch, loss: 0.004989893807459822\n",
      "24800th epoch, loss: 0.004968653158486494\n",
      "24850th epoch, loss: 0.0050130497244600504\n",
      "24900th epoch, loss: 0.005040083902623004\n",
      "24950th epoch, loss: 0.004928005070142982\n",
      "25000th epoch, loss: 0.005113575118136536\n",
      "25050th epoch, loss: 0.004872054539498691\n",
      "25100th epoch, loss: 0.004958094520824518\n",
      "25150th epoch, loss: 0.005014985380645692\n",
      "25200th epoch, loss: 0.004966201258266839\n",
      "25250th epoch, loss: 0.005321201085457404\n",
      "25300th epoch, loss: 0.004969235035321931\n",
      "25350th epoch, loss: 0.0049027327579760395\n",
      "25400th epoch, loss: 0.004871475659141773\n",
      "25450th epoch, loss: 0.004961930459240718\n",
      "25500th epoch, loss: 0.004769805068760317\n",
      "25550th epoch, loss: 0.004759381131091177\n",
      "25600th epoch, loss: 0.004888543925200079\n",
      "25650th epoch, loss: 0.004793865472964271\n",
      "25700th epoch, loss: 0.004760787460303042\n",
      "25750th epoch, loss: 0.004677822984008971\n",
      "25800th epoch, loss: 0.004721063085612852\n",
      "25850th epoch, loss: 0.004701176733346331\n",
      "25900th epoch, loss: 0.004899064393519556\n",
      "25950th epoch, loss: 0.004955024662535748\n",
      "26000th epoch, loss: 0.004618853473680587\n",
      "26050th epoch, loss: 0.004620495536516021\n",
      "26100th epoch, loss: 0.004710407525763989\n",
      "26150th epoch, loss: 0.004610365701766921\n",
      "26200th epoch, loss: 0.004736307405769822\n",
      "26250th epoch, loss: 0.004721239478778123\n",
      "26300th epoch, loss: 0.004635838033845977\n",
      "26350th epoch, loss: 0.004515474556904153\n",
      "26400th epoch, loss: 0.0046683352465919205\n",
      "26450th epoch, loss: 0.004489012053001822\n",
      "26500th epoch, loss: 0.004469248568120462\n",
      "26550th epoch, loss: 0.004501592844515782\n",
      "26600th epoch, loss: 0.004430906311411311\n",
      "26650th epoch, loss: 0.004504456469606617\n",
      "26700th epoch, loss: 0.0044595569820220305\n",
      "26750th epoch, loss: 0.004304014742204059\n",
      "26800th epoch, loss: 0.004235971537179828\n",
      "26850th epoch, loss: 0.004356312716127749\n",
      "26900th epoch, loss: 0.004271019563082068\n",
      "26950th epoch, loss: 0.004241475368544146\n",
      "27000th epoch, loss: 0.00424423174577618\n",
      "27050th epoch, loss: 0.004362274904123173\n",
      "27100th epoch, loss: 0.004295860949631766\n",
      "27150th epoch, loss: 0.00419648175686367\n",
      "27200th epoch, loss: 0.004224450128380015\n",
      "27250th epoch, loss: 0.004358780284196031\n",
      "27300th epoch, loss: 0.004184552818731528\n",
      "27350th epoch, loss: 0.004135500985690513\n",
      "27400th epoch, loss: 0.003993609922792091\n",
      "27450th epoch, loss: 0.003959501727927653\n",
      "27500th epoch, loss: 0.004055721587221326\n",
      "27550th epoch, loss: 0.004018138272857409\n",
      "27600th epoch, loss: 0.003945614001875577\n",
      "27650th epoch, loss: 0.003982997559815315\n",
      "27700th epoch, loss: 0.003839698013393389\n",
      "27750th epoch, loss: 0.0038079462516053433\n",
      "27800th epoch, loss: 0.003815252127286747\n",
      "27850th epoch, loss: 0.0037775371505092566\n",
      "27900th epoch, loss: 0.003729370800465179\n",
      "27950th epoch, loss: 0.003866132761410658\n",
      "28000th epoch, loss: 0.0037424752775069813\n",
      "28050th epoch, loss: 0.0036244296666502035\n",
      "28100th epoch, loss: 0.0037140822547911956\n",
      "28150th epoch, loss: 0.003657554485894923\n",
      "28200th epoch, loss: 0.003619519188842217\n",
      "28250th epoch, loss: 0.003609313419428331\n",
      "28300th epoch, loss: 0.0037406577735431667\n",
      "28350th epoch, loss: 0.003606701417818961\n",
      "28400th epoch, loss: 0.00361868702257801\n",
      "28450th epoch, loss: 0.003590265127722086\n",
      "28500th epoch, loss: 0.0036181541875880347\n",
      "28550th epoch, loss: 0.003458804238477463\n",
      "28600th epoch, loss: 0.003556253475056598\n",
      "28650th epoch, loss: 0.004205492789883371\n",
      "28700th epoch, loss: 0.003485880408693921\n",
      "28750th epoch, loss: 0.0034454858974748304\n",
      "28800th epoch, loss: 0.003463005625300928\n",
      "28850th epoch, loss: 0.00369429380764435\n",
      "28900th epoch, loss: 0.003606090233943925\n",
      "28950th epoch, loss: 0.0033955454334259425\n",
      "29000th epoch, loss: 0.003457967824573125\n",
      "29050th epoch, loss: 0.0035391802953347795\n",
      "29100th epoch, loss: 0.003455880928423926\n",
      "29150th epoch, loss: 0.003390217405054161\n",
      "29200th epoch, loss: 0.0034694492141803973\n",
      "29250th epoch, loss: 0.0035654744329294517\n",
      "29300th epoch, loss: 0.0034251966888749065\n",
      "29350th epoch, loss: 0.0033951444199747747\n",
      "29400th epoch, loss: 0.0034022772817093342\n",
      "29450th epoch, loss: 0.0033960837825003742\n",
      "29500th epoch, loss: 0.00358993018126597\n",
      "29550th epoch, loss: 0.0034028154340455165\n",
      "29600th epoch, loss: 0.003364509903175546\n",
      "29650th epoch, loss: 0.0032905554331666893\n",
      "29700th epoch, loss: 0.0033754287978032278\n",
      "29750th epoch, loss: 0.0035239618012937496\n",
      "29800th epoch, loss: 0.003254528822832822\n",
      "29850th epoch, loss: 0.003300114068752681\n",
      "29900th epoch, loss: 0.003312491178939765\n",
      "29950th epoch, loss: 0.0032768623972185772\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(n_epoch):\n",
    "\n",
    "    batch_list = make_batch(data, batch_size, window_size+1)\n",
    "    for batch_i, batch in enumerate(batch_list):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = np.array(batch)\n",
    "        batch_input = batch[:, :-1, :]\n",
    "        batch_output = batch[:, 1:, :]\n",
    "\n",
    "        batch_input = torch.tensor(batch_input, dtype=torch.float32)\n",
    "        batch_output = torch.tensor(batch_output, dtype=torch.float32)\n",
    "\n",
    "        if use_cuda:\n",
    "            batch_input = batch_input.cuda()\n",
    "            batch_output = batch_output.cuda()\n",
    "        \n",
    "        v, _ = model(batch_input)\n",
    "\n",
    "        loss = loss_fn(v, batch_output)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if ema_loss is None:\n",
    "            ema_loss = loss.item()\n",
    "        ema_loss = loss.item() * alpha + (1.-alpha) * ema_loss\n",
    "\n",
    "    if epoch_i % verbose_interval == 0:\n",
    "        print(f\"{epoch_i}th epoch, loss: {ema_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Inference\n",
    "model.eval()\n",
    "model.cpu()\n",
    "\n",
    "\n",
    "# Prepare train data distribution\n",
    "Z = []\n",
    "reconstruction_error = []\n",
    "\n",
    "v_diff_list = []\n",
    "\n",
    "batch_list = make_batch(data, batch_size, window_size, False)\n",
    "\n",
    "\n",
    "for batch_i, batch in enumerate(batch_list):\n",
    "    batch = np.array(batch)\n",
    "    batch_input = batch\n",
    "\n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.float32)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.float32)\n",
    "\n",
    "    v, z = model(batch_input)\n",
    "\n",
    "    Z.extend(z.tolist())\n",
    "    \n",
    "    v  # batch_size x widnow_size x n_feature\n",
    "    n_feature = v.shape[2]\n",
    "\n",
    "    v_pred = v[:, :-1, :]  # batch_size x (window_size-1) x n_feature\n",
    "    \n",
    "    v_gt = batch_input[:, 1:, :]\n",
    "    \n",
    "    v_diff = torch.abs(v_gt - v_pred) # batch_size x (window_size-1) x n_feature\n",
    "    \n",
    "    v_diff = v_diff.mean(dim=1) # batch_size  x n_feature\n",
    "    \n",
    "    v_diff = v_diff.view(-1, n_feature)  # batch_size x n_feature  => n_sample x n_feature\n",
    "    \n",
    "    v_diff_list.append(v_diff.detach().cpu().numpy())\n",
    "    \n",
    "    # v = v.view(-1, n_feature) # batch_size  x n_feature\n",
    "    batch_input.view(-1, n_feature)   \n",
    "    \n",
    "    # reconstruction_error.extend(torch.sum(torch.abs(v-batch_input), dim=[1,2]).detach().tolist())\n",
    "\n",
    "Z = np.array(Z)\n",
    "#reconstruction_error = np.array(reconstruction_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_diff_mat = np.concatenate(v_diff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264, 1578)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_diff_mat.shape  # n_sample x n_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = len(v_diff_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vector = v_diff_mat.mean(axis=0)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance= (1./n_sample)*np.dot((v_diff_mat-mean_vector).transpose(), v_diff_mat-mean_vector)  # [n_feature x n_sample] dot [n_sample x n_feature]  => n_featrue x n_feature  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = covariance + np.eye(*covariance.shape) # * 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.linalg.eigvals(covariance)>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264, 1578)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_diff_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_inv = np.linalg.inv(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_det = np.linalg.det(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(np.eye(*covariance.shape) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  1.27125184e-13,  1.71413083e-10, ...,\n",
       "        -8.64922886e-09, -5.83728453e-09, -7.09902346e-09],\n",
       "       [ 1.27125184e-13,  1.00000000e+00, -1.54718198e-10, ...,\n",
       "         2.52261609e-09,  4.05711132e-10,  7.50924694e-10],\n",
       "       [ 1.71413083e-10, -1.54718198e-10,  9.99994977e-01, ...,\n",
       "         2.01535553e-06, -2.57900452e-06, -6.95310206e-06],\n",
       "       ...,\n",
       "       [-8.64922886e-09,  2.52261609e-09,  2.01535553e-06, ...,\n",
       "         9.99824557e-01, -1.01223949e-04, -1.11868653e-04],\n",
       "       [-5.83728453e-09,  4.05711132e-10, -2.57900452e-06, ...,\n",
       "        -1.01223949e-04,  9.99859407e-01, -1.46720440e-04],\n",
       "       [-7.09902346e-09,  7.50924694e-10, -6.95310206e-06, ...,\n",
       "        -1.11868653e-04, -1.46720440e-04,  9.99776130e-01]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariance_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ckpt/multi_pdf.pickle\", \"wb\") as f:\n",
    "    pickle.dump({\"mean_vector\": mean_vector,\n",
    "                 \"covariance\": covariance, \n",
    "                 \"covariance_inv\": covariance_inv,\n",
    "                 \"covariance_det\": covariance_det}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Store\n",
    "torch.save(model.state_dict(), \"ckpt/model.ckpt\")\n",
    "\n",
    "with open(\"./ckpt/Z.pickle\", \"wb\") as f:\n",
    "    pickle.dump(Z, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ckpt/scaler.pickle\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open('ckpt/multi_pdf.pickle', 'rb') as file:\n",
    "         data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_vector': array([3.7111859e-06, 1.5762308e-06, 7.4348939e-03, ..., 2.2448460e-02,\n",
       "        2.1607760e-02, 2.5424536e-02], dtype=float32),\n",
       " 'covariance': array([[ 1.00000000e+00, -1.38174113e-13, -1.35951805e-10, ...,\n",
       "          9.45718082e-09,  6.57555832e-09,  7.99193778e-09],\n",
       "        [-1.38174113e-13,  1.00000000e+00,  1.51546428e-10, ...,\n",
       "         -2.62211697e-09, -4.79782214e-10, -8.41478831e-10],\n",
       "        [-1.35951805e-10,  1.51546428e-10,  1.00000507e+00, ...,\n",
       "         -1.70275621e-06,  2.98215059e-06,  7.51856987e-06],\n",
       "        ...,\n",
       "        [ 9.45718082e-09, -2.62211697e-09, -1.70275621e-06, ...,\n",
       "          1.00018470e+00,  1.09684210e-04,  1.22099183e-04],\n",
       "        [ 6.57555832e-09, -4.79782214e-10,  2.98215059e-06, ...,\n",
       "          1.09684210e-04,  1.00015119e+00,  1.60026291e-04],\n",
       "        [ 7.99193778e-09, -8.41478831e-10,  7.51856987e-06, ...,\n",
       "          1.22099183e-04,  1.60026291e-04,  1.00024093e+00]]),\n",
       " 'covariance_inv': array([[ 1.00000000e+00,  1.27125184e-13,  1.71413083e-10, ...,\n",
       "         -8.64922886e-09, -5.83728453e-09, -7.09902346e-09],\n",
       "        [ 1.27125184e-13,  1.00000000e+00, -1.54718198e-10, ...,\n",
       "          2.52261609e-09,  4.05711132e-10,  7.50924694e-10],\n",
       "        [ 1.71413083e-10, -1.54718198e-10,  9.99994977e-01, ...,\n",
       "          2.01535553e-06, -2.57900452e-06, -6.95310206e-06],\n",
       "        ...,\n",
       "        [-8.64922886e-09,  2.52261609e-09,  2.01535553e-06, ...,\n",
       "          9.99824557e-01, -1.01223949e-04, -1.11868653e-04],\n",
       "        [-5.83728453e-09,  4.05711132e-10, -2.57900452e-06, ...,\n",
       "         -1.01223949e-04,  9.99859407e-01, -1.46720440e-04],\n",
       "        [-7.09902346e-09,  7.50924694e-10, -6.95310206e-06, ...,\n",
       "         -1.11868653e-04, -1.46720440e-04,  9.99776130e-01]]),\n",
       " 'covariance_det': 1.5417414606094337}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
